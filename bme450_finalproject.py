# -*- coding: utf-8 -*-
"""BME450_finalproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VICCLX_QDz5TPsYYvu_dUg0hEz_qTl_6
"""

# Mount Google Drive before opening the file
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

!pip install torch torchvision rasterio opencv-python

import torch.nn as nn
import torch.optim as optim
import os
import glob
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import rasterio
from torchvision import transforms
import cv2
import pandas as pd
import torch.nn.functional as F
import matplotlib.pyplot as plt

"""**Check and Prepare Dataset**"""

# # Create datasets
# train_dataset = root_dir='/content/drive/MyDrive/BME_450_image_chips/training'
# val_dataset = root_dir='/content/drive/MyDrive/BME_450_image_chips/validate'
# test_dataset = root_dir='/content/drive/MyDrive/BME_450_image_chips/testing'

"""**Check that the Folders have the Same Amount of Files**"""

# # Compare the number of files in each folder
# def count_files_in_folders(root_dir):
#     file_counts = {}
#     if not os.path.isdir(root_dir):
#         print(f"Error: Directory '{root_dir}' not found.")
#         return file_counts

#     for subdir, _, files in os.walk(root_dir):
#         folder_name = os.path.relpath(subdir, root_dir)  # Get folder name relative to root
#         file_counts[folder_name] = len(files)

#     return file_counts


# # Example usage
# train_counts = count_files_in_folders('/content/drive/MyDrive/BME_450_image_chips/training')
# val_counts = count_files_in_folders('/content/drive/MyDrive/BME_450_image_chips/validation')
# test_counts = count_files_in_folders('/content/drive/MyDrive/BME_450_image_chips/testing')

# print("File counts in training:", train_counts)
# print("File counts in validation:", val_counts)
# print("File counts in testing:", test_counts)

"""**Check for File Corruption**"""

# # Check the images in the three folders to ensure they aren't corrupted
# def check_images(root_dir):
#     corrupted_images = []
#     for subdir, _, files in os.walk(root_dir):
#         for file in files:
#             if file.endswith(('.tif', '.tiff')):  # Check for GeoTIFF files
#                 image_path = os.path.join(subdir, file)
#                 try:
#                     with rasterio.open(image_path) as src:
#                         # Attempt to read the image data.  If it fails, it's likely corrupted
#                         src.read(1)  # Read only the first band for efficiency
#                 except rasterio.errors.RasterioIOError as e:
#                     corrupted_images.append(image_path)
#                     print(f"Error reading {image_path}: {e}")
#                 except Exception as e:  # Catch other potential errors
#                     corrupted_images.append(image_path)
#                     print(f"An unexpected error occurred while reading {image_path}: {e}")

#     if corrupted_images:
#         print("\nCorrupted image files found:")
#         for img in corrupted_images:
#             print(img)
#     else:
#         print("No corrupted images found.")

# # Example usage
# print("Checking training images:")
# check_images('/content/drive/MyDrive/BME_450_image_chips/training')

# print("\nChecking validation images:")
# check_images('/content/drive/MyDrive/BME_450_image_chips/validation')

# print("\nChecking testing images:")
# check_images('/content/drive/MyDrive/BME_450_image_chips/testing')

"""**Check Image Sizes**"""

# # Check the image sizes in all of the folders and print the largest size found
# def get_largest_image_size(root_dir):
#     largest_width = 0
#     largest_height = 0

#     for subdir, _, files in os.walk(root_dir):
#         for file in files:
#             if file.lower().endswith(('.tif', '.tiff')):
#                 image_path = os.path.join(subdir, file)
#                 try:
#                     with rasterio.open(image_path) as src:
#                         width = src.width
#                         height = src.height
#                         if width * height > largest_width * largest_height:
#                             largest_width = width
#                             largest_height = height
#                 except rasterio.errors.RasterioIOError as e:
#                     print(f"Error reading {image_path}: {e}")
#                 except Exception as e:
#                     print(f"An unexpected error occurred while reading {image_path}: {e}")
#     if largest_width == 0 and largest_height == 0:
#         return None
#     else:
#         return largest_width, largest_height

# train_largest = get_largest_image_size('/content/drive/MyDrive/BME_450_image_chips/training')
# val_largest = get_largest_image_size('/content/drive/MyDrive/BME_450_image_chips/validation')
# test_largest = get_largest_image_size('/content/drive/MyDrive/BME_450_image_chips/testing')

# print(f"Largest image in training: {train_largest}")
# print(f"Largest image in validation: {val_largest}")
# print(f"Largest image in testing: {test_largest}")

# # Find the absolute largest across all folders
# largest_overall = max(
#     (train_largest, "training"), (val_largest, "validation"), (test_largest, "testing"), key=lambda x: x[0][0] * x[0][1] if x[0] else 0 # Access the width and height from the first element of the tuple (x[0])
# )

# if largest_overall[0]:
#   print(f"\nOverall largest image size ({largest_overall[1]}): {largest_overall[0]}")
# else:
#   print("\nNo valid images found in any directory.")

# Check the image sizes in all of the folders and print the smallest size found
def get_smallest_image_size(root_dir):
    smallest_width = float('inf')  # Initialize to infinity
    smallest_height = float('inf')

    for subdir, _, files in os.walk(root_dir):
        for file in files:
            if file.lower().endswith(('.tif', '.tiff')):
                image_path = os.path.join(subdir, file)
                try:
                    with rasterio.open(image_path) as src:
                        width = src.width
                        height = src.height
                        if width * height < smallest_width * smallest_height:
                            smallest_width = width
                            smallest_height = height
                except rasterio.errors.RasterioIOError as e:
                    print(f"Error reading {image_path}: {e}")
                except Exception as e:
                    print(f"An unexpected error occurred while reading {image_path}: {e}")
    if smallest_width == float('inf') and smallest_height == float('inf'):
        return None
    else:
        return smallest_width, smallest_height

train_smallest= get_smallest_image_size('/content/drive/MyDrive/BME_450_image_chips/training')
val_smallest = get_smallest_image_size('/content/drive/MyDrive/BME_450_image_chips/validation')
test_smallest = get_smallest_image_size('/content/drive/MyDrive/BME_450_image_chips/testing')

print(f"Smallest image in training: {train_smallest}")
print(f"Smallest image in validation: {val_smallest}")
print(f"Smallest image in testing: {test_smallest}")

# Find the absolute smallest across all folders
smallest_overall = min(
    (train_smallest, "training"), (val_smallest, "validation"), (test_smallest, "testing"), key=lambda x: x[0][0] * x[0][1] if x[0] else 0 # Access the width and height from the first element of the tuple (x[0])
)

if smallest_overall[0]:
  print(f"\nOverall smallest image size ({smallest_overall[1]}): {smallest_overall[0]}")
else:
  print("\nNo valid images found in any directory.")

# Plot the intensity distributions of the image chips
import matplotlib.pyplot as plt
import rasterio
import os
import numpy as np

def plot_intensity_distributions(root_dir):
    """Plots the intensity distributions of image chips in a directory."""

    intensities = []
    for subdir, _, files in os.walk(root_dir):
        for file in files:
            if file.lower().endswith(('.tif', '.tiff')):
                image_path = os.path.join(subdir, file)
                try:
                    with rasterio.open(image_path) as src:
                        image_data = src.read(1) # Read the first band
                        intensities.extend(image_data.flatten())
                except Exception as e:
                    print(f"Error reading {image_path}: {e}")

    plt.figure(figsize=(8, 6))
    plt.hist(intensities, bins=50, range=(0, 255)) # Adjust range if needed
    plt.xlabel("Pixel Intensity")
    plt.ylabel("Frequency")
    plt.title(f"Intensity Distribution of Image Chips ({os.path.basename(root_dir)})")
    plt.show()


# For each folder:
plot_intensity_distributions('/content/drive/MyDrive/BME_450_image_chips/training')
plot_intensity_distributions('/content/drive/MyDrive/BME_450_image_chips/validation')
plot_intensity_distributions('/content/drive/MyDrive/BME_450_image_chips/testing')

"""**Check Geospatial Metadata of One File**"""

# with rasterio.open('/content/drive/MyDrive/BME_450_image_chips/training/snags_training/snags_managed_training_1.tif') as src:
#     print("CRS:", src.crs)
#     print("Bounds:", src.bounds)

# # Check the number of bands in one image
# with rasterio.open('/content/drive/MyDrive/BME_450_image_chips/training/snags_training/snags_managed_training_1.tif') as src:
#     print("Number of bands:", src.count)

"""**Create Histogram of the Image Sizes in Two Subfolders**"""

# Plot the histogram of the image sizes in the two subfolders "snags" and "trees" under the main folder /content/drive/MyDrive/BME_450_image_chips/training
def get_image_sizes(root_dir):
    sizes = []
    for subdir, _, files in os.walk(root_dir):
        for file in files:
            if file.lower().endswith(('.tif', '.tiff')):
                image_path = os.path.join(subdir, file)
                try:
                    with rasterio.open(image_path) as src:
                        sizes.append(src.width * src.height)
                except Exception as e:
                    print(f"Error reading {image_path}: {e}")
    return sizes


snags_sizes = get_image_sizes('/content/drive/MyDrive/BME_450_image_chips/training/snags_training')
trees_sizes = get_image_sizes('/content/drive/MyDrive/BME_450_image_chips/training/trees_training')

plt.figure(figsize=(10, 6))
plt.hist(snags_sizes, bins=20, alpha=0.5, label='Snags')
plt.hist(trees_sizes, bins=20, alpha=0.5, label='Trees')
plt.xlabel('Image Size (pixels)')
plt.ylabel('Number of Images')
plt.title('Histogram of Image Sizes')
plt.legend()
plt.show()

"""**Resize Images to 38x14**"""

import gc
import shutil

def resize_image_opencv_safe(image_path, output_path, output_shape=(14, 38)):
    try:
        with rasterio.open(image_path) as src:
            image = src.read()  # shape: (bands, height, width)
            profile = src.profile.copy()

        bands, _, _ = image.shape
        resized_bands = []

        for b in range(bands):
            band = image[b, :, :].astype(np.float32)
            band_resized = cv2.resize(band, (output_shape[1], output_shape[0]), interpolation=cv2.INTER_LINEAR)
            resized_bands.append(band_resized.astype(np.float32))  # use float32 throughout

        resized_image = np.stack(resized_bands)

        profile.update(
            height=output_shape[0],
            width=output_shape[1],
            transform=rasterio.transform.from_origin(0, 0, 1, 1),  # neutral dummy transform
            dtype='float32'
        )

        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        with rasterio.open(output_path, 'w', **profile) as dst:
            dst.write(resized_image)

        print(f"√ Resized: {image_path} → {output_path}") # Alt 251

    except Exception as e:
        print(f"x Error resizing {image_path}: {e}")

def get_output_path(image_path, base_input_dir, base_output_dir):
    # Get relative path from base_input_dir
    rel_path = os.path.relpath(image_path, base_input_dir)
    # Split into parts
    parts = rel_path.split(os.sep)
    # Map old subfolders to new ones
    if 'snags_training' in parts:
        parts[parts.index('snags_training')] = 'snags_resized'
    elif 'snags_validation' in parts:
        parts[parts.index('snags_validation')] = 'snags_resized'
    elif 'snags_testing' in parts:
        parts[parts.index('snags_testing')] = 'snags_resized'
    elif 'trees_training' in parts:
        parts[parts.index('trees_training')] = 'trees_resized'
    elif 'trees_validation' in parts:
        parts[parts.index('trees_validation')] = 'trees_resized'
    elif 'trees_testing' in parts:
        parts[parts.index('trees_testing')] = 'trees_resized'
    # Join new path
    new_rel_path = os.path.join(*parts)
    return os.path.join(base_output_dir, new_rel_path)

input_dir = "/content/drive/MyDrive/BME_450_image_chips"
output_dir = "/content/drive/MyDrive/BME_450_image_chips_resized_10x10"

batch_size = 10
batch = []

for root, _, files in os.walk(input_dir):
    for file in files:
        if file.endswith(".tif"):
            image_path = os.path.join(root, file)
            output_path = get_output_path(image_path, input_dir, output_dir)
            resize_image_opencv_safe(image_path, output_path)
            gc.collect()

"""**Resize Images to 12x12**"""

import os
import numpy as np
import cv2
import rasterio
import gc

def resize_image_opencv_safe(image_path, output_path, output_shape=(12, 12)):
    try:
        with rasterio.open(image_path) as src:
            image = src.read()
            profile = src.profile.copy()

        bands, _, _ = image.shape
        resized_bands = []

        for b in range(bands):
            band = image[b, :, :].astype(np.float32)
            band_resized = cv2.resize(band, (output_shape[1], output_shape[0]), interpolation=cv2.INTER_LINEAR)
            resized_bands.append(band_resized.astype(np.float32))

        resized_image = np.stack(resized_bands)

        profile.update(
            height=output_shape[0],
            width=output_shape[1],
            transform=rasterio.transform.from_origin(0, 0, 1, 1),
            dtype='float32'
        )

        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        with rasterio.open(output_path, 'w', **profile) as dst:
            dst.write(resized_image)

        print(f"√ Resized: {image_path} → {output_path}")

        # Explicitly delete large arrays
        del image, resized_bands, resized_image
        gc.collect()

    except Exception as e:
        print(f"x Error resizing {image_path}: {e}")

def get_output_path(image_path, base_input_dir, base_output_dir):
    # Get relative path from base_input_dir
    rel_path = os.path.relpath(image_path, base_input_dir)
    # Split into parts
    parts = rel_path.split(os.sep)
    # Map old subfolders to new ones
    if 'snags_training' in parts:
        parts[parts.index('snags_training')] = 'snags_resized'
    elif 'snags_validation' in parts:
        parts[parts.index('snags_validation')] = 'snags_resized'
    elif 'snags_testing' in parts:
        parts[parts.index('snags_testing')] = 'snags_resized'
    elif 'trees_training' in parts:
        parts[parts.index('trees_training')] = 'trees_resized'
    elif 'trees_validation' in parts:
        parts[parts.index('trees_validation')] = 'trees_resized'
    elif 'trees_testing' in parts:
        parts[parts.index('trees_testing')] = 'trees_resized'
    # Join new path
    new_rel_path = os.path.join(*parts)
    return os.path.join(base_output_dir, new_rel_path)

input_dir = "/content/drive/MyDrive/BME_450_image_chips"
output_dir = "/content/drive/MyDrive/BME_450_image_chips_resized_10x10"

batch_size = 10
batch = []

for root, _, files in os.walk(input_dir):
    for file in files:
        if file.endswith(".tif"):
            image_path = os.path.join(root, file)
            output_path = get_output_path(image_path, input_dir, output_dir)
            resize_image_opencv_safe(image_path, output_path)
            gc.collect()

"""**Check Image Sizes Again**"""

# Check the image sizes in all of the folders and print the largest size found
def get_largest_image_size(root_dir):
    largest_width = 0
    largest_height = 0

    for subdir, _, files in os.walk(root_dir):
        for file in files:
            if file.lower().endswith(('.tif', '.tiff')):
                image_path = os.path.join(subdir, file)
                try:
                    with rasterio.open(image_path) as src:
                        width = src.width
                        height = src.height
                        if width * height > largest_width * largest_height:
                            largest_width = width
                            largest_height = height
                except rasterio.errors.RasterioIOError as e:
                    print(f"Error reading {image_path}: {e}")
                except Exception as e:
                    print(f"An unexpected error occurred while reading {image_path}: {e}")
    if largest_width == 0 and largest_height == 0:
        return None
    else:
        return largest_width, largest_height

train_largest = get_largest_image_size('/content/drive/MyDrive/BME_450_image_chips_resized_10x10/training')
val_largest = get_largest_image_size('/content/drive/MyDrive/BME_450_image_chips_resized_10x10/validation')
test_largest = get_largest_image_size('/content/drive/MyDrive/BME_450_image_chips_resized_10x10/testing')

print(f"Largest image in training: {train_largest}")
print(f"Largest image in validation: {val_largest}")
print(f"Largest image in testing: {test_largest}")

# Find the absolute largest across all folders
largest_overall = max(
    (train_largest, "training"), (val_largest, "validation"), (test_largest, "testing"), key=lambda x: x[0][0] * x[0][1] if x[0] else 0 # Access the width and height from the first element of the tuple (x[0])
)

if largest_overall[0]:
  print(f"\nOverall largest image size ({largest_overall[1]}): {largest_overall[0]}")
else:
  print("\nNo valid images found in any directory.")

"""**Define Dataset**"""

class GeoTIFFDataset(Dataset):
    "Custom dataset for loading GeoTIFF images from folders with subfolders as classes"

    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.classes = sorted(os.listdir(root_dir))
        self.class_to_index = {cls: idx for idx, cls in enumerate(self.classes)} # {self.classes[i]: i for i in range(len(self.classes))}
        self.image_paths = []
        self.labels = []

        for class_name in self.classes:
            class_dir = os.path.join(root_dir, class_name)
            if os.path.isdir(class_dir):
                for image_path in glob.glob(os.path.join(class_dir, '*.tif')):
                    self.image_paths.append(image_path)
                    self.labels.append(self.class_to_index[class_name])

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        try:
          with rasterio.open(img_path) as src:
            image = src.read()  # Read all bands
        except Exception as e:
          print(f"Error reading {img_path}: {e}")
          return None, None # Handle corrupted images
        label = self.labels[idx]

        if self.transform:
            image = self.transform(image) # Now applying transform to the NumPy array
        return image, label

"""**Calculate Mean and STD for Normalization**"""

def calculate_mean_std(root_dir, clip_min=0, clip_max=10000):  # Adjust clip values
    """Calculates the mean and standard deviation of images in a directory,
    separately for each band, with clipping to handle extreme values."""

    pixel_values_per_band = [[] for _ in range(3)]  # Assuming 3 bands

    for subdir, _, files in os.walk(root_dir):
        for file in files:
            if file.endswith(('.tif', '.tiff')):
                image_path = os.path.join(subdir, file)
                try:
                    with rasterio.open(image_path) as src:
                        image = src.read()  # Read all bands

                        # Clip pixel values for each band
                        for band_idx in range(image.shape[0]):
                            clipped_band = np.clip(image[band_idx], clip_min, clip_max)
                            pixel_values_per_band[band_idx].extend(clipped_band.flatten())
                except Exception as e:
                    print(f"Error reading {image_path}: {e}")

    means = [np.mean(band_values) for band_values in pixel_values_per_band]
    stds = [np.std(band_values) for band_values in pixel_values_per_band]

    return means, stds

# Example usage with clipping:
train_means, train_stds = calculate_mean_std('/content/drive/MyDrive/BME_450_image_chips_resized_10x10/training', clip_min=0, clip_max=10000)  # Adjust clip values as needed

print(f"Training set means (per band): {train_means}")
print(f"Training set stds (per band): {train_stds}")

# import numpy as np
# import os
# import rasterio
# import matplotlib.pyplot as plt

# def calculate_mean_std(root_dir, clip_min=None, clip_max=None):  # Optional clipping
#     """Calculates the mean and standard deviation of images in a directory,
#     separately for each band, with optional clipping and histogram plotting."""

#     pixel_values_per_band = [[] for _ in range(3)]  # Assuming 3 bands

#     for subdir, _, files in os.walk(root_dir):
#         for file in files:
#             if file.endswith(('.tif', '.tiff')):
#                 image_path = os.path.join(subdir, file)
#                 try:
#                     with rasterio.open(image_path) as src:
#                         image = src.read()  # Read all bands

#                         # Optional clipping
#                         if clip_min is not None and clip_max is not None:
#                             for band_idx in range(image.shape[0]):
#                                 image[band_idx] = np.clip(image[band_idx], clip_min, clip_max)

#                         for band_idx in range(image.shape[0]):
#                             pixel_values_per_band[band_idx].extend(image[band_idx].flatten())

#                 except Exception as e:
#                     print(f"Error reading {image_path}: {e}")

#     means = [np.mean(band_values) for band_values in pixel_values_per_band]
#     stds = [np.std(band_values) for band_values in pixel_values_per_band]

#     # Plot histogram for each band
#     for band_idx, band_values in enumerate(pixel_values_per_band):
#         plt.figure(figsize=(8, 6))
#         plt.hist(band_values, bins=50)  # Adjust bins as needed
#         plt.title(f"Histogram of Pixel Values (Band {band_idx + 1})")
#         plt.xlabel("Pixel Value")
#         plt.ylabel("Frequency")
#         plt.show()

#         print(f"Band {band_idx + 1}:")
#         print(f"  Minimum value: {np.min(band_values)}")
#         print(f"  Maximum value: {np.max(band_values)}")
#         print(f"  Mean: {means[band_idx]}")
#         print(f"  Standard Deviation: {stds[band_idx]}")

#     return means, stds

# # Example usage:
# train_means, train_stds = calculate_mean_std('/content/drive/MyDrive/BME_450_image_chips_resized/training')

# Define transforms (adjust normalization based on GeoTIFF data range)
# transformed = transforms.Compose([
#     transforms.Lambda(lambda img: img.astype(np.float32)),  # Ensure float32 type
#     transforms.Lambda(lambda img: np.transpose(img, (1, 2, 0))),  # (C, H, W) to (H, W, C)
#     transforms.ToTensor(),  # Convert to tensor (C, H, W)
#     transforms.Normalize(mean=train_means, std=train_stds) # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # For 3 bands
# ])
transformed = transforms.Compose([
    transforms.Lambda(lambda img: np.nan_to_num(img, nan=0.0, posinf=0.0, neginf=0.0)),  # Clean up NaNs/Infs
    transforms.Lambda(lambda img: np.clip(img, 0, 10000)),  # Clip extreme pixel values
    transforms.Lambda(lambda img: img.astype(np.float32)),
    transforms.Lambda(lambda img: np.transpose(img, (1, 2, 0))),  # (C, H, W) -> (H, W, C)
    transforms.ToTensor(),  # (H, W, C) -> (C, H, W) and scales [0, 1] if float
    transforms.Normalize(mean=train_means, std=train_stds),
])

# Create datasets
train_dataset = GeoTIFFDataset(root_dir='/content/drive/MyDrive/BME_450_image_chips_resized_10x10/training', transform=transformed)
val_dataset = GeoTIFFDataset(root_dir='/content/drive/MyDrive/BME_450_image_chips_resized_10x10/validation', transform=transformed)
test_dataset = GeoTIFFDataset(root_dir='/content/drive/MyDrive/BME_450_image_chips_resized_10x10/testing', transform=transformed)

# Plot the intensity distributions of the image chips

import matplotlib.pyplot as plt
import numpy as np

def plot_intensity_distributions_dataset(dataset, title):
    intensities = []
    for i in range(len(dataset)):
        image, _ = dataset[i]
        if image is not None:  # Check for corrupted images
          intensities.extend(image.numpy().flatten())

    plt.figure(figsize=(8, 6))
    plt.hist(intensities, bins=50)  # Adjust range if needed
    plt.xlabel("Pixel Intensity")
    plt.ylabel("Frequency")
    plt.title(f"Intensity Distribution of Image Chips ({title})")
    plt.show()

# Plot for each dataset
plot_intensity_distributions_dataset(train_dataset, "Training Set")
plot_intensity_distributions_dataset(val_dataset, "Validation Set")
plot_intensity_distributions_dataset(test_dataset, "Test Set")

# !apt-get install gdal-bin
# !gdalinfo /content/drive/MyDrive/BME_450_image_chips/training/snags/snags_managed_training_655.tif

# # Search for and print images that contain NAN values in the three above folders
# def find_nan_images(root_dir):
#     nan_images = []
#     for subdir, _, files in os.walk(root_dir):
#         for file in files:
#             if file.endswith(('.tif', '.tiff')):
#                 image_path = os.path.join(subdir, file)
#                 try:
#                     with rasterio.open(image_path) as src:
#                         image = src.read()
#                         if np.isnan(image).any():
#                             nan_images.append(image_path)
#                             print(f"Found NaN values in: {image_path}")
#                 except Exception as e:
#                     print(f"Error processing {image_path}: {e}")
#     return nan_images

# # Example usage for all three folders
# print("Searching for NaN values in training images...")
# find_nan_images('/content/drive/MyDrive/BME_450_image_chips_resized/training')

# print("\nSearching for NaN values in validation images...")
# find_nan_images('/content/drive/MyDrive/BME_450_image_chips_resized/validation')

# print("\nSearching for NaN values in testing images...")
# find_nan_images('/content/drive/MyDrive/BME_450_image_chips_resized/testing')

"""**Using 38x14-sized Images**"""

# import torch.nn.functional as F

# # Define a CNN model
# class GeoCNN(nn.Module):
#     def __init__(self, num_classes):
#         super(GeoCNN, self).__init__()
#         # Input: 3-channel RGB (38x14 after resizing)
#         self.conv1 = nn.Conv2d(3, 16, 3, padding=1) # 16x14x38
#         self.pool1 = nn.MaxPool2d(2, 2) # 16x7x19
#         self.bn1 = nn.BatchNorm2d(16)

#         self.conv2 = nn.Conv2d(16, 32, 3, padding=1)  # 32x7x19
#         self.pool2 = nn.MaxPool2d(2, 2) # 32x3x9
#         self.bn2 = nn.BatchNorm2d(32)

#         # Calculate final features: 32 channels * 3 height * 9 width
#         self.fc1 = nn.Linear(32 * 3 * 9, 128)  # Adjusted input size
#         self.fc2 = nn.Linear(128, num_classes)

#         # Add regularization
#         self.dropout = nn.Dropout(0.5)

#     def forward(self, x): #F.relu and F.leaky.relu
#         x = self.pool1(F.leaky_relu(self.bn1(self.conv1(x)))) # 16x7x19, try ELU since I'm getting NaN loss values
#         # print("Conv1 out:", x.min().item(), x.max().item())
#         x = self.pool2(F.leaky_relu(self.bn2(self.conv2(x)))) # 32x3x9
#         # print("Conv2 out:", x.min().item(), x.max().item())

#         x = x.view(x.size(0), -1) # Flatten
#         x = F.relu(self.fc1(x))
#         x = self.dropout(x)
#         x = self.fc2(x)
#         return x

"""**Using 12x12-sized Images**"""

class GeoCNN(nn.Module):
    def __init__(self, num_classes):
        super(GeoCNN, self).__init__()
        # Input: 3-channel RGB (12x12 after resizing)
        self.conv1 = nn.Conv2d(3, 16, 3, padding=1) # 16x12x12
        self.pool1 = nn.MaxPool2d(2, 2) # 16x5x5
        self.bn1 = nn.BatchNorm2d(16)

        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)  # 32x5x5
        self.pool2 = nn.MaxPool2d(2, 2) # 32x2x2
        self.bn2 = nn.BatchNorm2d(32)

        # Calculate final features: 32 channels * 2 height * 2 width
        self.fc1 = nn.Linear(288, 128)  # Adjusted input size
        self.fc2 = nn.Linear(128, num_classes)

        # Add regularization
        self.dropout = nn.Dropout(0.5)

    def forward(self, x): #F.relu and F.leaky.relu
        # print("Input to conv1:", x.min().item(), x.max().item(), x.mean().item())
        x = self.pool1(F.leaky_relu(self.bn1(self.conv1(x)))) # 16x5x5, try ELU since I'm getting NaN loss values
        # print("Conv1 out:", x.min().item(), x.max().item())
        x = self.pool2(F.leaky_relu(self.bn2(self.conv2(x)))) # 32x2x2
        # print("Conv2 out:", x.min().item(), x.max().item())

        x = x.view(x.size(0), -1) # Flatten
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# Initialize the model, loss function, and optimizer
num_classes = len(train_dataset.classes)
model = GeoCNN(num_classes)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)

train_losses = []
val_losses = []
test_losses = []
val_accuracies = []
test_accuracies = []

# Training loop
num_epochs = 10
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Define data loaders
# This is the missing part that caused the error
batch_size = 32  # Adjust as needed
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

def evaluate(model, data_loader, criterion, device):
    model.eval()
    total_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in data_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    return total_loss / len(data_loader), 100. * correct / total

# for epoch in range(num_epochs):
#     model.train()
#     for inputs, labels in train_loader:
#         inputs, labels = inputs.to(device), labels.to(device)
#         optimizer.zero_grad()
#         outputs = model(inputs)
#         loss = criterion(outputs, labels)
#         loss.backward()
#         optimizer.step()

#     # Validation
#     val_loss, val_acc = evaluate(model, val_loader, criterion, device)

#     # Testing
#     test_loss, test_acc = evaluate(model, test_loader, criterion, device)

#     print(f'Epoch {epoch+1}/{num_epochs}, '
#           f'Train Loss: {loss.item():.4f}, '
#           f'Val Loss: {val_loss:.4f}, '
#           f'Test Loss: {test_loss:.4f}, ')
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Optional safety
        optimizer.step()
        running_loss += loss.item() * inputs.size(0)

    avg_train_loss = running_loss / len(train_loader.dataset)
    train_losses.append(avg_train_loss)

    val_loss, val_acc = evaluate(model, val_loader, criterion, device)
    test_loss, test_acc = evaluate(model, test_loader, criterion, device)

    val_losses.append(val_loss)
    test_losses.append(test_loss)
    val_accuracies.append(val_acc)
    test_accuracies.append(test_acc)

    print(f"Epoch {epoch+1}/{num_epochs}, "
          f"Train Loss: {avg_train_loss:.4f}, "
          f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, "
          f"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%")

import matplotlib.pyplot as plt

# Plot losses
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.plot(test_losses, label='Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss Trends')
plt.legend()
plt.grid(True)
plt.show()

# Plot accuracies
plt.figure(figsize=(10, 5))
plt.plot(train_accuracies, label='Train Accuracy')
plt.plot(val_accuracies, label='Validation Accuracy')
plt.plot(test_accuracies, label='Test Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.title('Accuracy Trends')
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
import torch.nn.functional as F

# Put model in eval mode
model.eval()

# Get a batch from the test loader
inputs, labels = next(iter(test_loader))
inputs = inputs.to(device)

# Forward pass
with torch.no_grad():
    outputs = model(inputs)
    probs = F.softmax(outputs, dim=1).cpu().numpy()  # Convert to probabilities
    preds = outputs.argmax(dim=1).cpu().numpy()

# Plot results
num_images = min(8, inputs.size(0))
fig, axes = plt.subplots(1, num_images, figsize=(15, 4))
for i in range(num_images):
    img = inputs[i].cpu().numpy().transpose(1, 2, 0)
    axes[i].imshow(img[:, :, 0], cmap='gray')  # Or .mean(axis=2) for grayscale
    axes[i].set_title(f'Pred: {preds[i]}\nProb: {probs[i][preds[i]]:.2f}')
    axes[i].axis('off')
plt.tight_layout()
plt.show()

def visualize_feature_maps(model, input_image):
    model.eval()
    input_image = input_image.unsqueeze(0).to(device)  # Add batch dim

    with torch.no_grad():
        x = F.leaky_relu(model.bn1(model.conv1(input_image)))
        feature_maps = x.squeeze(0).cpu().numpy()

    # Plot first 8 feature maps
    fig, axes = plt.subplots(1, 8, figsize=(20, 5))
    for i in range(8):
        axes[i].imshow(feature_maps[i], cmap='viridis')
        axes[i].set_title(f'Feature {i}')
        axes[i].axis('off')
    plt.tight_layout()
    plt.show()

# Example usage:
img, label = train_dataset[0]
visualize_feature_maps(model, img)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

def evaluate_with_predictions(model, loader):
    model.eval()
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(device), y.to(device)
            out = model(x)
            pred = out.argmax(dim=1)
            all_preds.extend(pred.cpu().numpy())
            all_labels.extend(y.cpu().numpy())
    return all_labels, all_preds

labels, preds = evaluate_with_predictions(model, test_loader)
cm = confusion_matrix(labels, preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=train_dataset.classes)
disp.plot(cmap='Blues')
plt.show()

# Calculate User's and Producer's accuracy
from sklearn.metrics import accuracy_score

# Assuming 'labels' and 'preds' are already calculated from evaluate_with_predictions
user_accuracy = accuracy_score(labels, preds)
print(f"User Accuracy: {user_accuracy}")

# Producer's accuracy is calculated per class
producer_accuracies = []
for i in range(len(cm)):
  producer_accuracies.append(cm[i,i] / cm[i,:].sum())

print("Producer Accuracies per Class:")
for i, acc in enumerate(producer_accuracies):
    print(f"{train_dataset.classes[i]}: {acc}")

overall_producer_accuracy = np.mean(producer_accuracies)  # Overall average
print(f"\nOverall Producer Accuracy: {overall_producer_accuracy}")

# Save model predictions with confidence levels
import matplotlib.pyplot as plt
import torch.nn.functional as F

model.eval()
examples = []
max_examples = 4  # number to display

with torch.no_grad():
    for inputs, labels in test_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        probs = F.softmax(outputs, dim=1)
        preds = outputs.argmax(dim=1)

        for i in range(inputs.shape[0]):
            img = inputs[i].cpu().numpy().transpose(1, 2, 0)
            pred = preds[i].item()
            true = labels[i].item()
            confidence = probs[i][pred].item()

            if len(examples) < max_examples:
                examples.append((img, pred, true, confidence))

        if len(examples) >= max_examples:
            break

# Plot examples
fig, axes = plt.subplots(1, len(examples), figsize=(12, 3))

class_names = train_dataset.classes  # ['snags', 'trees'] or similar

for i, (img, pred, true, conf) in enumerate(examples):
    axes[i].imshow(img[:, :, 0], cmap='gray')  # Show one band (or mean if you want RGB)

    correct = (pred == true)
    label = f"{'✅' if correct else '❌'} {class_names[pred]} ({conf*100:.1f}%)"
    if not correct:
        label += f"\nTrue: {class_names[true]}"

    axes[i].set_title(label, fontsize=10)
    axes[i].axis('off')

cir_img = np.dstack([
    img[:, :, 4],  # NIR -> Red channel
    img[:, :, 3],  # Red -> Green channel
    img[:, :, 2]   # Green -> Blue channel
])

# Optionally, normalize for display
cir_img = cir_img / cir_img.max()

plt.tight_layout()
plt.show()

examples = {
    "correct_snag": None,
    "correct_tree": None,
    "wrong_snag": None,
    "wrong_tree": None
}

with torch.no_grad():
    for inputs, labels in test_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        probs = F.softmax(outputs, dim=1)
        preds = outputs.argmax(dim=1)

        for i in range(inputs.shape[0]):
            img = inputs[i].cpu().numpy().transpose(1, 2, 0)
            pred = preds[i].item()
            true = labels[i].item()
            conf = probs[i][pred].item()

            # Fill each category once
            if pred == true:
                if true == 0 and examples["correct_snag"] is None:
                    examples["correct_snag"] = (img, pred, true, conf)
                elif true == 1 and examples["correct_tree"] is None:
                    examples["correct_tree"] = (img, pred, true, conf)
            else:
                if true == 0 and examples["wrong_snag"] is None:
                    examples["wrong_snag"] = (img, pred, true, conf)
                elif true == 1 and examples["wrong_tree"] is None:
                    examples["wrong_tree"] = (img, pred, true, conf)

        if all(v is not None for v in examples.values()):
            break

labels_to_show = ["correct_snag", "correct_tree", "wrong_snag"] # , "wrong_tree"

fig, axes = plt.subplots(1, len(labels_to_show), figsize=(14, 3))

for idx, key in enumerate(labels_to_show):
    img, pred, true, conf = examples[key]
    correct = pred == true
    pred_label = train_dataset.classes[pred]
    true_label = train_dataset.classes[true]

    label = f"{'√' if correct else 'x'} {pred_label} ({conf*100:.1f}%)"
    if not correct:
        label += f"\nTrue: {true_label}"

    axes[idx].imshow(img[:, :, 0], cmap='gray')  # or use .mean(axis=2) for grayscale average
    axes[idx].set_title(label, fontsize=10)
    axes[idx].axis('off')

plt.tight_layout()
plt.show()